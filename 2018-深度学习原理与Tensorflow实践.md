# 深度学习原理与Tensorflow实践

这本书的作者都非名校毕业，再联想到现在 Matrix 的团队。最近真的感触很深，这个世界并非只属于“第一”。每个人都在自己圈子里发光，很多时候只是需要接纳自己并“自信”、“勇敢”的迎接挑战，最后“坚持”去做，就足够了。  

1. 深度学习简介
    - 深度学习介绍：人工智能时代，AI 的兴起，机器学习问题（Regression, Classification, Representation, Feature），人工智能>机器学习>深度学习
    - 深度学习的趋势：数据越来越大、计算性能越来越好、模型越来越复杂
2. TensorFlow 系统介绍
    - TensorFlow 的诞生动机
        - Google Brain，指在探索深度学习在实际场景中的应用。
        - 几点核心需求：要具有灵活的表达能力，能够快速实现各种算法；高执行性能，具备分布式扩展性；跨平台可移植性；实验可复现性；支持快速产品化，模型可随时部署。
    - TensorFlow 系统简介
    - TensorFlow 基础概念
        - 计算图、Session 会话（驱动TF系统执行计算交互的入口）
    - 系统架构
        - client -> master -> worker 分布式系统。
        - tf.Session 接口与 master 进行通信，并向 master 提交触发执行的请求，master 将执行任务分配到一个或多个 worker 进程上，执行的结果通过 master 返回给客户端。
        - 计算设备（device）是 TensorFlow 系统中对计算资源定义的基础单位。
    - 源码结构
        - 执行系统的实现借鉴了 NumPy 等高性能数值计算库的方案，使用 C++ 编写，以充分保证执行性能。
        - TF 是 2015 年 11 月提出的。
3. Hello TensorFlow
    - 环境准备
        - 在操作系统方面，TensorFlow 主要支持 UNIX 内核的操作系统，对于 Windows 系统的支持不全面。
        - GPU 服务器是深度学习研究中不可或缺的部分。理想情况下，可以在单机环境中用小规模数据完成代码调试，调试通过以后，在高性能的 GPU 服务器（或服务器集群）上使用大数据对模型进行训练。
        - Linux GPU 服务器安装
            1. GPU 的选择：对于科学计算而言，决定性能最关键的指标是 GPU 的浮点运算能力。对于目前主流的深度学习应用来说，单精度浮点运算能力是核心需求。Tesla（优秀，但是贵），Geforce GTX（游戏主打），Titan X 是计算单元最多、性能最高的一款产品。注意在配置服务器时，推荐选择同系列显卡中性能最高的产品。
            2. 服务器的供电和散热是需要重点关注的方面。
            3. 除了 GPU 以外的其他部分，如 GPU、内存、硬盘、网络带宽等，没有特别要求，达到普通服务器的水平就基本不会构成性能的瓶颈。
            4. 操作系统推荐 Ubuntu or Cent OS
            5. 常用的库：NumPy, pandas, Matplotlib, PIL, IPython & Jupyter, scikit-learn, OpenCV. 
        - Titanic 题目实战
            - **TensorFlow 代码中提供了一个工具程序可以用于查看 checkpoint 文件的内容，代码在 tensorflow/python/tools/inspect_checkpoint.py**
            - **需要注意的是，Saver 对象在初始化时，若不指定变量列表，默认只会自动收集其声明之前的所有变量**，在 Saver 对象初始化后的所有变量将不被记录，上面代码中的 v2 变量就不会在存取的范围内。这样的机制在迁移学习的应能中非常有用，例如要将一个基于 ImageNet 数据训练好的 CNN 应用在新类型图片识别上，只需要加载模型卷积部分的参数，重新训练最后的全连接网络即可。
        - TensorBoard 可视化
        - 数据读取：
            - 选择合适的数据格式，也是一个需要着重关注的方面
            - 文件格式：npy 和 npz，pkl，hdf
            - 对于大数据：TFRecord
        - SkFlow, TFLearn, TF-Slim
4. CNN "看懂" 世界
    - 卷积神经网络(Convolutional Neural Networks, CNNs), 纽约大学 Yann LeCun，1989.
    - 2012 年 ILSVRC 图像识别竞赛，Alex Krizhevsky 基于卷积神经网络设计的分类模型 AlexNext 压倒性优势赢得了当年的冠军，由此深度学习得到热力追捧。
    - 图像识别的难题
        1. 图像分类与目标定位（CLS-LOC）：
            - 分类：top-5错误率评估。2012-AlexNet, 2016-公安部第三研究所选派的“搜神”（Trimps-Soushen）代表队获得冠军。
            - 目标定位：要在分类正确的基础上，从图片中标识出目标物体所在的位置，用方框框定，错误率作为评判。2015-ResNet，2016-Trimps-Soushen.
        2. 目标检测（DET）：
            - 在定位的基础上更进一步，在图片中同时检测并定位多个类别的物体。平均检出率mean AP（mean Average Precision）作为评判标准。
        3. 视频目标检测（VID）：
            - 检测视频每一帧包含的多个类别的物体。最大难度在于要求算法的检测效率非常高。2015年南京信息工程大学队伍获冠军。
        4. 场景分类（Scene）：
            - 要识别图片中的场景，比如森林、剧场、会议室、商店等。该项目由 MIT Places 团队组织。
            - 还有一个子问题是场景分割，是要将图片划分成不同的区域。
        5. 其他有趣的问题：
            - 看图说话（Image Captioning），Google 让计算机自己从无标注数据中学会了识别猫的样子。
     - CNNs 的基本原理：
         - 充分利用了图片中相邻区域的信息，通过稀疏连接和共享权重的方式大大减少参数矩阵的规模，从而减少计算量，也大大地提高了手链速度。
         - 卷积的数学意义：`$ s(t) = \int x(a)w(t-a)da  $`，**卷积操作实际上是对输入函数的每一个位置进行加权累加。`$x(t)$` 是输入信号，衰减系数 `$ w(t-a) $` 是系统对信号的响应，卷积 `$ (x*w)(t) $` 就是在时刻 `$t$` 对系统观察的结果**，是所有信号经过系统的处理后的结果的叠加。
         - 卷积滤波：卷积其实是图像处理中一种常用的线性滤波方法，使用卷积可以达到图像**降噪、锐**化等多种滤波效果。 `$ S(i,j) = (I*K)(i, j) = \sum_{m}\sum_{n}I(m,n)K(i-m, j-n)$`
         - CNNs 中的卷积层：局部感知域、权值共享、多核卷积。
         - 池化 (Pooling)：一般采用 `$2*2$` 的窗口大小，max pooling/average pooling，降维 + 平移不变性 (translation invariant)。
         - ReLU：
             - sigmoid：
                 - 优势：导数计算简单，可以用于拟合概率
                 - 劣势：梯度消失
             - ReLU：
                 - 避免了梯度消失问题，保证参数能持续收敛。AlexNet 的论文中也提到，对于同一个网络结构，使用 ReLU 作为激活函数，其收敛速度要比使用 tanh 快 6 倍以上。
        - 多层卷积
        - Dropout：训练的时候，随机让一部分隐层节点失效，这样就达到了改变网络结构的目的，但每个节点的权重都会被保留下来。在最终预测时，打开全部隐层节点，使用完整的网络进行计算，就相当于把多个不同结构的网络组合在了一起。AlexNet 将 dropout 应用在网络中的前两个全连接层。虽然需要两倍的迭代次数才能收敛，但是若不用 dropout 的话就会导致严重的过拟合。
    - 经典的 CNN 模型：AlexNet, GoogLeNet, VGGNet, ResNet.
        - 2012-ILSVRC（冠军）, AlexNet, Geoffrey Hinton 教授的学生 Alex Krizhevsky。着重提到 ReLU 和 dropout 起到了非常重要的作用，使用 ReLU 大大加快收敛速度，比 tanh 快 6 倍。而 dropout 达到了防过拟合的效果。嫌弃深度学习热潮。
        - 2014-ILSVRC（分类第二，定位第一）, VGGNets。以 AlexNet 为基础，尝试建立了一个层次更多、深度更深的网络。最主要区别在于，VGGNet 的每个卷积层并不是只做一次卷积操作，而是连续卷积2～4次。（VGG16, VGG19）。最主要的贡献是展示了当卷积网络的深度不断加深时，可以提高准确率。
        - 2014-ILSVRC（分类第一），GoogLeNet & Inception，与 VGGNets 平分秋色。对于卷积核大小的选择需要经验和大量实验才能确定，`$ 3*3, 5*5, 7*7 $` 哪一个好？Inception 的做法跳出直线加深网络层数的思路，通过增加“宽度”的方式增加网络的复杂度。在一个卷积层中，并行使用 `$3*3, 5*5, 7*7$` 卷积和池化，同时提取不同尺度的特征，然后通过 `$1*1$` 的卷积对每一个分支进行降维，最后将结果合并拼接在一起。不仅减少了参数的数量，而且多多种尺度特征的适应性更好了。Inception-v2 在第一代的基础上加入了 Batch Normalization，可以说是 Local Response Normalization 的升级版，是加快收敛速度的利器。Inception-v3，最核心的思想是将卷积操作继续分解成更小的卷积核。
        - 2016-微软亚洲研究院的何凯明-ResNets，斩获 ILSVRC-2015 的5项冠军. 其最大的贡献就是通过设计一种残差网络的结构，避免了随着网络层数加深而产生的梯度消失或梯度爆炸问题，不但使深度神经网络的收敛速度更快、精度更高，而且让加深网络深度成为可能。解决“深度网络的退化问题（degradation problem）”。`$ F(x) = H(x) - x $`，假设求解 `$F(x)$`会比求`$H(x)$`要简单一些的话，可以通过求解`$F(x)+x$`来达到最终的目标。
        - 图像风格迁移，2015 NIPS 会议上，德国图宾根大学的 Leon A. Gatys。
5. RNN “能说会道”
    - 自然语言处理（Natural Language Processing, NLP）同样是人工智能领域的研究热点。语言分为语音和文字量大部分。和人类考量“听说读写”四方面能力相似，人工智能领域发展出了如下四个最主要的研究方向，听：语音识别（Speech To Text, STT），实现从音频到文本转换；说：语音合成（Text To Speech, TTS），实现从文本到音频转换；读：文本理解（Text Understanding），实现文本到语义转换；写：文本生成（Text Generation），实现语义到文本转换。因为文本数据具有易保存、噪声小、自带标签等特性，使得它更适合被用于分析语义概念。相比之下，语音数据存储量大、噪音处理复杂、数据类别标记困难。这导致语音识别和语音合成技术侧重于信号分类还原。
    - 文本理解和文本生成问题
        - 文本理解：又称为文本语义分析。它是基于词法、语法等信息，让计算机自动从文本中挖掘出有用的信息，并帮助人们理解文本内容的智能算法。典型的应用包括**文本分类、情感分析、自动文摘**等。
        - 在循环神经网络中，每一个输出元素的生成都基于同一个网络，这样就可以简单地输出变长结果。输出序列的每个元素都是其之前位置的输出元素所组成的函数，这就保持了序列元素的顺序依赖关系。
    - 标准 RNN 模型
        - RNN 网络就是在传统神经网络的基础上加入了“记忆”的成分。
        - RNN 模型介绍：
            - `$ a_{t} = b + Wh_{t-1} + Ux_{t}, h_{t} = tanh(a_{t}), o_{t} = c + Vh_{t}, \hat{y}_{t} = softmax(o_{t}) $`
            - 注意：
                - 参数共享（Parameter sharing）在 RNN 中也起到了至关重要的作用。
                - RNN 的缩写不要混淆，循环神经网络（Recurrent Neural Network）和递归神经网络（Recursive Neural Network）不同。
        - BPTT 算法
            - BPTT (Back-Propagation Through Time) 算法是 BP 算法在 RNN 结构上的一种变体形式，采取在 RNN 模型的展开形式上进行梯度计算。
        - 灵活的 RNN 结构：
            - one to one: 传统神经网络和CNN
            - one to many: 英文释义
            - many to one: sentiment analysis
            - many to many (Encoder-Decoder): machine translation
            - many to many (synchronization): 词性标注
        - TensorFlow 实现正弦序列预测
    - LSTM 模型
        - 虽然理论上 RNN 能够处理长期依赖（Long-Term Dependenncy）的问题，但从实践来看，普通的 RNN 模型并不能学习到远距离的知识。
        - 长期依赖的难题：`$ h_{t} = Wh_{t-1} = W^{2}h_{t-2} = ... = W^{t}h_{0} $`
        - LSTM 基本原理：
            - Sepp Hochreiter 和 Jurgen Schmidhuber 于 1997 年首次提出，解决长期依赖问题。
            - 最主要的改进就是多出了三门控制器：input gate, output gate, forget gate. 
            -  `$ C_{t} = f_{t}C_{t-1} + i_{t}*C', f_{t} = \sigma(W_{f}[h_{t-1}, x_{t}] + b_{f}), i_{t} = \sigma(W_{i}[h_{t-1}, x_{t}] + b_{i}), C'=tanh(W_{c}[h_{t-1}, x_{t}] + b_{C}) $`
     - TensorFlow 构建 LSTM 模型
     - 更多 RNN 的变体
         - Bidirectional RNNs, Deep Recurrent Networks. 
     - 语言模型
         - 语言模型就是用于评估文本符合语言使用习惯程度的模型。
         - NGram 语言模型
             - 基于马尔可夫假设，即：假设一段文本，第 n 个单词出现的概率只与前面有限的 n-1 个词相关，而与其他词都不相关。
             - 基于这样一种假设，可以评估文本中每一个词出现的概率，那么这整段文本的合法程度可以通过统计每 n 个词在语料库中出现的可能性来评估，整段文本的概率就是各个词出现概率的乘积。
         - 神经网络语言模型
             - 传统的 NGram 语言模型存在较大问题
                 - 一方面，由于数据的稀疏性导致估算能力有限，仅能对两三个词的长度的序列进行评估；
                 - 另一方面，由于 NGram 语言模型是离散模型，语义相似甚至一致的两个词对于语言模型来说是完全不同的。
             - 为了解决以上问题，Yoshua Bengio 等人在 2003 年通过引入词向量概念，提出了基于神经网络的神经网络语言模型（NNLM, Neural Network Language Model）。
         - 循环神经网络语言模型
             - 对于深度学习算法来说，要面临的一大核心问题就是如何减少参数个数。语言模型也不例外，Bengio 在神经网络语言模型的论文中就有提出，可以利用循环神经网络来降低模型参数个数。2010年内，由 Tomas Mikolov 在 Recurrent Neural network based language model 论文中提出了循环神经网络语言模型（Recurrent Neural Network Language Model, RNNLM）。
         - 语言模型也能写代码（example）
         - 改进方向
             - 一方面，从词的表示来说，词表的选择可以是单词粒度或者字符粒度；词表规模是一个影响循环神经网络性能的重要因素，针对这一方面也有很多针对性的改进工作，如合并低频词为 rare token、词聚类、LightRNN等技术。词向量也需要考虑，静态映射（one-hot）或动态映射（word embedding）。
             - 另一方面，从模型结构来说。
         - 对话机器人。
             - Introduction
                 - 对话机器人（Chatbot）是指能够使用自然语言进行智能对话的软件系统。比如 Query-Answer System, Dialogue System 等。
                 - 虽然科学家尚未完全理解人类是如何学习第一语言的整个过程。但学习过第二语言的人们都明白，语言学习分为多个阶段而且是一个漫长的过程。[me, 有第二语言才能真正理解自己的母语]。
                 - 无论处于什么语言学习阶段。词汇量都是语言学习的关键基础。没有足够的词汇量，就无法熟练流畅使用语言。
             - 对话机器人的发展（基于规则、基于检索、基于生成模型）
                 1. 基于规则的对话系统：字符串匹配查找，正则表达；
                 2. 基于检索的对话系统：2014 年华为的研发工程师曾发表了一种 3 级结构的短消息对话系统。
                 3. 基于生成模型：Seq2Seq
             - 基于 seq2seq 的对话机器人
                 - 训练数据：华为诺亚方舟实验室公开的数据为例，Post-Comment Pair
6. CNN+LSTM看图说话
    1. CNN + LSTM 网络模型与图像检测问题
        - 与图像分类问题相比较，图像检测除了需要判断图像内容的类型之外，还需要标定图像中所有目标所在的位置。
        - 输入到损失函数中的基本样本特征是网络子图特征。
        - CNN + LSTM 网格结构主要用以解决遮挡目标的图像检测问题。
            - OverFeat 是使用 CNN 来解决图像检测问题的著名算法，诞生自纽约大学著名教授 Yann LeCun 所领导的实验室，并获得 ILSVRC2013 挑战赛图像检测问题冠军。最大的贡献在于提出使用深度卷积网络模型呢一次完成分类、定位和检测三个计算机视觉任务。但有遮挡时效果不好。
            - Faster R-CNN (Regions with CNN features)，是2015年COCO图像检测竞赛的基准对比算法。
            - ReInspect 算法是在 OverFeat 算法的基础上使用 LSTM 网络优化了遮挡目标的检测问题。
            - TensorBox 是一种呢基于 TensorFlow 的、可训练的图像检测识别框架。
        - CNN + LSTM 网络模型与图像摘要问题
            - 图像摘要问题：图像内容准确提取 + 图像内容的恰当文字转换
            - 图像摘要问题的求解过程可以理解为一种概率联合分布的转换模型。对于图像 I 和其对应的描述问题本`$[S_{begin}, S_{1}, ..., S_{i}, ..., S_{end}]$`，训练的目标为图像摘要的最大概率密度函数`$ P(S_{begin}, S_{1}, ..., S_{end} | I) $`。
            - Oriol Vinyals 等提出 NIC 图像摘要生成算法。
7. 损失函数与优化算法
    - 损失函数的作用就是衡量逼近程度。
    - 对于机器学习问题来说，给出量化指标，确定优化方向，是解决问题的第一步。从这个意义上说，损失函数的设计体现了人们对待求解问题的理解程度。
    - 常用的优化算法：Stochastic Gradient Descent (SGD), Momentum, ADAM（使用迭代次数作为参数对梯度均值和梯度均值方差进行矫正，迭代次数增加，准确率越高，应该缩小更新步长）, RMSProp（对不同模型权重设置不同的学习步长）；另外，针对大数据训练，TF 提供了类别采样（Candidate Sampling）损失函数。