# Machine Learning In Action - Peter Harrington

## 写在前面

作者写本书的灵感来源于论文[Top 10 algorithms in data mining](https://link.springer.com/article/10.1007/s10115-007-0114-2). 2007 年发表于 Journal of Knowledge and Information Systems. 这 10 个算法分别是：C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, 和 CART。10 个算法覆盖了聚类、分类、统计学习、关联关系学习和联系挖掘（link mining）任务。与时俱进，关注领域动态很关键。

全书由四个部分构成，即分类、回归、无监督学习和其他工具。第一部分介绍了机器学习机器基础，其中包括机器学习的概念、关键术语、主要任务、算法选择、开发流程以及主语 Python 的简要介绍；介绍了常用的分类算法，包括 kNN, 决策树和基于统计的分类方法-Naive Bayes，Loistic 回归，支持向量机，AdaBoost。第二部分介绍数值回归（线性回归，局部线性加权，岭回归，lasso）和树回归。第三部分介绍无监督学习，内容包括聚类：k-means，关联关系挖掘：Apriori，FP-growth 加速发现频繁项集。第四部分介绍其他工具，包括 PCA 和 SVD 数据规约方法，大数据与 MapReduce 的应用。最后附录还总结了阅读全书所需要的基础知识，包括 Python 入门，线性代数，概率论复，以及一些课外学习资源。

本书从实践者的视角进行编写，省略了很多算法的复杂理论和推导，好处是我们能更直观的从应用角度体会各个数据挖掘算法的核心思想和应用流程。我认为对于初学者这是一本不错的入门书籍，对于工程师或成熟的算法从业者这是一本不错的工具书。实践类书籍，不动手，收获不会很大，因为信息量少！我通读全书的同时利用python notebook 的形式进行练习，获益良多，尤其是对实现细节有了更多的了解。[练习源码](https://github.com/hfl15/MLinAction)。虽然我们实际工作中大多调用现有库，但重新实现一遍算法有助于对算法逻辑的理解。

当然如果希望通过本书就能深入的学习数据挖掘或机器学习领域是远远不够的，我们还需要继续从理论和实践两个方面不断的积累和充实自己。

三个细节得到的 inspiration。

- 作者 Peter Harrington，偶然接触到“随机”一词，对兴趣的深入探索使得他发现了新大陆，由此改变自己人生轨迹的同时也收获了知识带来的满足与幸福。**不要放过让你心动的细节，后续那是上帝给我们开启新人生的钥匙**
- 译者王斌老师的序，落款时间是凌晨，或许就是坚守。
- 书本封面的插画，赞颂计算机产业所具有的创造性、主动性和趣味性。**点滴的积累与沉淀会使得你的世界更有深度**。

## 目录

[第一部分 分类](#01)

* [1. 机器学习基础](#01-01)
* [2. KNN](#01-02)
* [3. 决策树](#01-03)
* [4. 基于概率论的分类方法：朴素贝叶斯](#01-04)
* [5. Logistic 回归](#01-05)
* [6. 支持向量机](#01-06)
* [7. 利用 AdaBoost 元算法提高分类性能](#01-07)

[第二部分 利用回归预测数值型数据](#02)

* [8. 预测数值型数据：回归](#02-01)
* [9. 树回归](#02-02)

[第三部分 无监督学习](#03)

* [10. 利用 K-mean 算法对未标注数据分组](#03-01)
* [11. 使用 Apriori 算法进行关联分析](#03-02)
* [12. 使用 FP-growth 算法来高效发现频繁项集](#03-03)

[第四部分 其他工具](#04)

* [13. 利用 PCA 来简化数据](#04-01)
* [14. 利用 SVD 简化数据](#04-02)
* [15. 大数据与 MapReduce](#04-03)

----


<h2 id="01"> 第一部分 分类</h2>

<h3 id="01-01"> 1. 机器学习基础 </h3>

简单来说，机器学习就是将无序的数据转换成有用的信息。机器学习解决的问题往往是 NP 问题，因此获取精确的解是不可能的，因此也就有了从统计上对结果进行近似求解。

传感器和海量数据为机器学习提供了很好的基础，Data Driven 的方法逐渐将人类从繁琐的领域知识中解脱出来。

机器学习非常重要：我们每时每刻都在接触大量的免费信息，如何理解数据、从中抽取有价值的信息才是其中的关键。如何分析数据，展示数据、交流和利用数据，从中获取有价值的信息尤为重要。

关键术语：专家系统，特征，属性，实例，数值型，二值型，枚举类型，分类，算法训练，训练集，训练样本，目标变量，类别，测试数据，知识表示。

**机器学习的任务：**

- supervised：
   - classification: 目标变量是离散的
   - regression : 目标变量是连续的 
- un-supervised
   - clustering 
   - 关联关系挖掘

**算法的选择：**

- 必需考虑的两个问题
   1. 使用机器学习算法的目的，想要算法完成什么任务
   2. 数据的问题，需要分析或收集的数据是什么  
      - 对实际数据了解越充分，越容易创建符合实际需求的应用程序：特征是离散还是连续？特征中是否有缺失？什么原因造成的缺失？数据中是否存在异常？某个特征发生的频率如何？等等
- 反复试错的过程：
   - 只能在一定程度上缩小算法的空间 
   - 没有绝对好的完美的算法，要综合考虑各方面的因素以及算法的效率，最好是多方对比后进行选择 

**开发及其学习程序的步骤：**

1. 收集数据
2. 准备输入数据：某些后续的分析过程和算法模型要求特定格式的输入
3. 分析输入数据
4. 训练算法
5. 测试算法：如果结果不好再返回前面的步骤进行调整
6. 使用算法


#### python

- python 优势：python 的语法清晰；易于操作纯文本文件；使用广泛，存在大量文档。
- python 常用的库：NumPy, SciPy, Matplotlib 等
- python 语言特点：清晰易懂，可以让使用者更关注算法层面的思考
- python 的缺点：相比于 c++ 和 java 效率不好 （唯一的缺点），但这个缺点不影响全局，最关键还是在于正确的算法思想。

<h3 id="01-02"> 2. KNN </h3>

- 算法概述
    - 优点：精度高、对异常值不敏感、无数据输入假设
    - 缺点：计算复杂度高、空间复杂度高、无法给出任何数据的基础结构信息
    - 特点：基于实例的学习，没有训练过程
    - 适用范围：数值型和标称型
- 算法描述
    - 训练：存储所有数据
    - 测试：对未知数据 x 进行如下步骤
        1. 计算 x 到训练集中所有样例的距离 dist
        2. 对 dist 进行排序
        3. 选取距离最近的 k 个点
        4. 确定 x 的类标：常用多数表决
- 两个例子：改进约会网站的配对效果、手写识别系统
- 实践经验
    - 距离函数：KNN 的核心，如何计算空间中两个样本的距离，常用的 p 距离，局部加权，高斯核等。
    - 数据归一化：量纲不同对结果影响很大
    - 留 10% 数据作为测试


<h3 id="01-03"> 3. 决策树 </h3>

- 算法概述
    - 优点：计算复杂度不高，输出结果容易理解，对缺失值不敏感，可以处理不相关特征数据
    - 缺点：可能产生过拟合问题（剪枝可以缓解该问题）
    - 适用数据类型：数值型和标称型
- 理论基础：信息论（信息增益，熵），属性划分是关键。
- 实现细节：
    - 预测隐形眼镜类别
    - Matploblib 注解功能，图示决策树
    - 剪枝 （样本太少就不再继续分）
    - 实现 ID3，还有C4.5 和 CART
    - 判别式模型，每个样本沿树根到叶子结点有确切的分类

<h3 id="01-04"> 4. 基于概率论的分类方法：朴素贝叶斯</h3>

- 算法概述
    - 优点：在数据较少时仍有效，可处理多分类问题
    - 缺点：对于数据的预处理方式敏感
    - 特点：生成式模型，学习分类的概率
    - 适用数据类型：标称型数据
- 理论基础：贝叶斯概率论
    - 贝叶斯定理：`$ p(c | x) = \frac{p(x | c) * p(c)}{ p (x)} $`
    - 全概率公式：`$ p(x) = \sum_{c} p(x | c)  $`
    - 条件独立性假设进化计算（朴素）：`$ p(x_{1},..x_{n} | c) = p(x_{1} | c)*...*p(x_{n} | c)$`
    - 确定分类： `$ argmax_{c} p(c |x) \approx  argmax_{c} p(x | c) * p(c) \approx argmax_{c} p(x_{1} | c)*...*p(x_{n} | c)p(c)$`
    - 降低计算复杂性，避免溢出，取 log，将乘法变成加法，于此同时单调性不变。
- 实践
    - 
    - 实例
        - 解析 RSS 源数据
        - 过滤垃圾邮件
        - 个人广告中获取区域倾向

<h3 id="01-05"> 5. Logistic 回归 </h3>

- 算法概述
    - 优点：计算代价不高，容易理解和实现
    - 缺点：容易欠拟合，分类精度可能不高
    - 特点：实际应用中常见，高维稀疏特征 + Logistic 回归，比如点击率预估问题。
    - 适用数据类型：数值型和标称型数据
- 理论基础
    - sigmoid 函数：`$\sigma (z) = \frac{1}{1+e^{-z}}, 0 < \sigma (z) < 1 $`
    - logist regression: `$\hat{y} = \sigma(w_{0}x_{0} + ... + w_{n}x_{n}) = \sigma(W^{T}X)$`
    - 梯度上升最优化算法（常说的是梯度下降算法，梯度是函数上升最快的方向，训练时找最小值点，更新时取梯度的反方向。）, `$ w = w - \alpha * \bigtriangleup f(x),  \bigtriangleup f(x) $` 是梯度。 
- 实现细节
    - 梯度上升法的实现（梯度下降，随机梯度下降）
    - 关键参数：学习率（learning rate）`$\alpha$`
    - 作图：参数-迭代次数，可观察各个参数的收敛情况。
    - 例子：预测马死亡率，分类

<h3 id="01-06"> 6. 支持向量机 </h3>

- 支持向量机的理论相对较复杂，正确实现完整的算法很难，一般使用 LIBSVM 库。
- 算法概述
    - 优点：泛化误差率低，计算开销不大，结果易解释；
    - 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于二分类问题
    - 特点：最大间隔分类
    - 适用数据类型：数值型和标称型
- 理论基础：
    - 重要概念：线性可分/线性不可分，分类超平面，间隔（margin），支持向量，
    - 最大化margin，拉格朗日乘子转为对偶问题（理论知识参考书籍，但可以参考更理论的书籍）
    - 核函数：径向基函数（radial basis function）最流行的核函数。
- SMO 高效优化算法：
    - 核心思想：成对优化参数
- 实践
    - 应用化版 SMO 算法处理小规模数据集
    - Platt SMO 算法加速优化
    - 实例：手写识别问题回顾。

<h3 id="01-07"> 7. 利用 AdaBoost 元算法提高分类性能 </h3>

- 算法概述
    - 优点：泛化误差率低，易编码，可以应用在大部分分类器上，无参数调整
    - 缺点：对李群点敏感
    - 适用数据类型：数值型和标称型数据
    - 特点：
        - 集成算法（ensemble method）或元算法（meta-algorithm）。使用集成算法可以有多种方式：不同算法集成、同一算法不同设置下集成、数据集不同部分分配给不同分类器后集成。
- 理论基础：
    - 基于数据集多重样的分类器：
        - bagging (bootstrap aggregating)：有放回采样（构成 s 个与原数据集大小相同的数据集），比如 random forest.
        - boosting：关注被已有分类器错分的数据来获得新的分类器。（训练数据有不同的权重，错分次数越多，权重越大）。比如 AdaBoost。
- 实践：
    - 基于单层决策树构建弱分类器
        -单层决策树（decision stump，也称为树桩）
    - 完整实现 AdaBoost
    - 几点：
        - 确保类别标签是 +1 和 -1 而非 1 和 0 
        - max(error, 1e-16)，用于确保在没有错误时不会发生溢出。（实现中会经常遇到这样的技巧。）
        - 很多人认为 AdaBoost 和 SVM 是监督学习中最强大的两种方法。
            - 这两者之间有不少相似之处。可以把弱分类器想象成 SVM 的一个核函数，可以按照最大化某个最小间隔的方式重写 AdaBoost 算法。
            - 不同之处在于：各自定义的间隔计算方式有所不同，导致结果不同。尤其是在高维空间下，这两者之间的差异会更明显。
- 补充知识：
    - 过拟合现象
    - 非均衡分类问题：
        - 使用其他评估指标：正确率（precision）、召回率（recall）、ROC 曲线
        - 基于代价函数的分类器决策控制。代价敏感学习（cost-sensitive learning）可以处理非均衡分类问题。
        - 数据采样方法：
            - 欠采样（undersampling）：缺点是不知道该剔除那些样本，被剔除的样本有可能包含其他样本没包含的关键信息。常剔除那些距离分类面远的样例。
            - 过采样（oversampling）：对少的数据进行复制。
            - 前两种的混合方法。
        

<h2 id="02">第二部分 利用回归预测数值型数据</h2>

<h3 id="8"> 8. 预测数值型数据：回归 </h3>

- 算法概述：
    - 优点：结果易于理解，计算上不复杂
    - 缺点：对非线性的数据拟合不好
    - 适用数据类型：数值型和标称型数据
- 理论基础：
    - 优化方法：普通最小二乘法（ordinary least squares）, OLS，逆矩阵不存在时不可使用。
    - 测量：误差平方和最小
    - 评估指标：R<sup>2</sup>
    - 局部加权线性回归：
        - 线性回归有可能出现欠拟合
        - 但局部加权线性回归也存在缺点，就是每次必须在整个数据集上运行。也就是说为了做出预测，必须保存所有的训练数据。
    - 岭回归(ridge regression) vs lasso regression
- 技巧：
    - 误差分析：权衡偏差和方差
    - 实例：预测乐高玩具套装的价格

<h3 id="9"> 9. 树回归 </h3>

当数据拥有很多特征并且特征间的关系非常复杂时，构建全局模型的想法显得很难。再者实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任何数据。

分类回归树 CART（Classification And Regression Trees）的树构建算法。该算法在每个叶子结点上都构建一个线性模型，由此可以对复杂的非线性特征进行学习。

- 算法概述：
    - 优点：可以对复杂和非线性的数据建模
    - 缺点：结果不易理解
    - 使用数据类型：数值型和标称型数据
- 连续和离散型特征的树的构建
- 缓解过拟合：树剪枝（预剪枝，后剪枝） 

<h2 id="03"> 第三部分 无监督学习 </h2>

<h3 id="10"> 10. 利用 K-mean 算法对未标注数据分组 </h3>

全自动分类、cluster identification、unsupervised classification、

- 优点：容易实现
- 缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢
- 使用数据类型：数值类型
- 关键参数：距离度量函数（一般根据数据特性来选），k 的选择 （根据任务、数据特性、效果等因素猜）
- 收敛标准：所有数据点的簇分配结果不再改变未知；当 loss（各个簇内点到簇心的平均距离）小于某个阈值。
- 进一步优化
    - k-means 问题：
        - 收敛到局部最小而非全局最小
        - 使用 SSE(Sum of Squared Error， 误差平方和)对效果进行度量，SSE 越小，说明点越接近质心。误差取了平方，因此指标更重视那些远离中心的点。
        - 一种肯定能降低 SSE 值的方法是增加簇的个数，但这违背了聚类的目标——维持簇数目不变。
    - 后处理提高聚类性能：
        - 将具有最大 SSE 值的簇划分成两个簇
        - 合并最近的质心，或者合并两个使得 SSE 增幅最小的质心
    - 二分 k-means 算法（bisecting k-means）
        - 根据降低 SSE 的程度进行划分（类似二叉树逐渐划分开）
- 示例：对地图上的点进行聚类    
        
<h3 id="11"> 11. 使用 Apriori 算法进行关联分析 </h3>

- 定义：从大规模数据集中寻找物品间的隐含关系被称作**关联分析（association analysis）**或者**关联规则学习（association rule learning）**
- 问题：寻找物品的不同组合是一项十分耗时的任务，需要很高的计算代价，蛮力搜索不可行。如何更智能的搜索频繁项集成为了关键问题。**Apriori** 是其中一种经典的算法。
- Apriori
    - 优点：易编码实现
    - 缺点：在大数据集上可能较慢
    - 适用数据类型：数值型或者标称型
- 术语：频繁项集（frequent item sets），关联规则（association rules），支持度，可信度，项集，最小支持度。
- 经典例子：尿布与啤酒
- Apriori 原理（可有效减少感兴趣的项集）：如果某个集是频繁的，那么它的所有子集也是频繁的。也就是说如果一个项集是非频繁集，那么它的所有超集也是非频繁的。
- 实现细节：
    - frozenset 而不是 set：frozenset 不可变，之后可以用来作字典的键值，但 set 不能达到这一目的。
    - python 不能创建只有一个整数的集合，因此这里必须使用列表实现项集
- 示例：
    - 发现国会投票中的模式
    - 发现毒蘑菇的相似特征

<h3 id="12"> 12. 使用 FP-growth 算法来高效发现频繁项集 </h3>

- 优点：一般快于 Apriori
- 缺点：实现比较困难，在某些数据集上性能会下降
- 适用数据类型：标称型数据
- 示例：
    - 发现 Twitter 源中的共现词（co-occuring word）
    - 从新闻网站点击流中挖掘


<h2 id="04">第四部分 其他工具 </h2>

<h3 id="13"> 13. 利用 PCA 来简化数据 </h3>

降维技术（dimensionality reduction）：

- 易可视化
- 使数据集更容易使用
- 降低很多算法的计算开销
- 去除噪音
- 使得结果易懂

主流技术：

- PCA（主成分分析，Principal Component Analysis），目前应用最为广泛
    - 核心思想：将数据映射到新的坐标系，新坐标系的选择由数据特性决定。逐一选择数据中方差最大的方向（各个方向相互垂直（正交，orthogonal））
    - 优点：降低数据的复杂性，识别最重要的多个特征。
    - 缺点：不一定需要，且可能损失游泳信息。
    - 适用数据类型：数值型数据
- 因子分析（Factor Analysis）
    - 假设：
        - 在观察数据的生成中有一些观察不到的**隐变量（latent variable）**
        - 观察数据是这些隐变量和某些噪声的线性组合，也就是说找到隐变量可以实现**降维**
- 独立成分分析（Independent Component Analysis, ICA）
    - 假设：
        - 数据从 N 个数据源产生的（和因子分析类似）
        - 数据是多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在 PCA 中只假设数据是不相关的。和因子分析一样，此时产生数据的数据源个数少于观察数据的数目，则可实现降维过程。
        

<h3 id="14"> 14. 利用 SVD 简化数据 </h3>

- 奇异值分解（Singular Value Decomposition, SVD）
    - 从生物信息学到金融学等在内的很多应用中，SVD 都是提取信息的强大工具
    - SVD 能够用小得多的数据集来表示原始数据集
    - SVD 从有噪声数据中抽取相关特征。
- 优点：简化数据，去除噪声，提高算法的结果。
- 缺点：数据的转换可能难以理解。
- 适用数据类型：数值型数据。
- SVD 的应用：
    - 隐性语义索引（Lantent Semantic Indexing, LSI）或隐性语义分析（Latent Semantic Analysis, LSA）。
    - 推荐系统。
- 矩阵分解：
    - 不同的矩阵分解技术具有不同的性质，其中有些更适合某个应用，有些则更适合其他应用。最常见的一种矩阵分解技术是 SVD。
    - `$ Data_{m\ast n} = U_{m * m}\Sigma _{m*n}V_{n*n}^{T} $`

<h3 id="15"> 15. 大数据与 MapReduce </h3>

<h3 0d="16"> 16. 资源 </h3>

- 常用公开数据集：http://archive.ics.uci.edu/ml/index.php
- 亚马逊公开的大数据：https://aws.amazon.com/public-datasets/
- 美国政府：
    - 公开数据集：https://www.data.gov/
    - 还维护了美国州、城市和国家等网站内超链接列表，提供了类似的开放数据：https://www.data.gov/open-gov/
- Infochimps 公司公开数据集，有一些需要购买：http://www.infochimps.com/
- Data Wrangling 是一个私人博客，提供网络上大量数据连接，很久没更新了：https://www.trifacta.com/

